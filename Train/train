import json
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
import torch.nn as nn
import math
from torch.utils.data import DataLoader
from model import BiRNN
from data_util import *

if __name__ == '__main__':

# -------------------------------------------------------------------------------------------------
    # 显卡
    # os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # os.environ["CUDA_VISIBLE_DEVICES"] = "0, 1, 2"
    # device_ids = [0, 1, 2]
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print('device: ', device)

# -------------------------------------------------------------------------------------------------
    # 参数
    model_params = {}
    model_dir = "../Model_ccs/save_model"
    train_data = "../data/train_data.pkl"
    test_data = "../data/test_data.pkl"

    model_params['lab_name'] = 'label'
    model_params['num_input'] = 32
    model_params['timesteps'] = 66
    model_params['num_hidden'] = 256
    model_params['num_layers'] = 2
    model_params['num_classes'] = 1
    model_params['dropout_keep_prob'] = 0.9
    model_params['use_uncertainty'] = False  #
    model_params['simple'] = False  #
    model_params['num_tasks'] = -1  #
    model_params['batch_size'] = 64
    model_params['model_dir'] = model_dir
    model_params['lr_base'] = 0.01
    model_params['training_steps'] = 2000  # 55000
    model_params['reduce_lr_step'] = 1500  # 50000
    model_params['eval_interval'] = 100
    model_params['early_stop'] = 400
    model_params['train_file'] = train_data
    model_params['test_file'] = test_data
    early_stop_count = 0
    print(model_params)


    with open('../Model_ccs/model_params.txt', 'w') as f:
        json.dump(model_params, f)

#-------------------------------------------------------------------------------------------------
    # 模型
    model = BiRNN(model_params['num_layers'],
                  model_params['num_hidden'],
                  model_params['num_classes'],
                  model_params['dropout_keep_prob']).to(device)
    # model = nn.DataParallel(model, device_ids).to(device)

# -------------------------------------------------------------------------------------------------
    # 预训练
    pretrain = False
    if pretrain:
        model.load_state_dict(torch.load('../Model_ccs/model_pretrian'))
        print('get pretrain!')
# -------------------------------------------------------------------------------------------------
    # 数据集
    dataset, dataset_test = get_data_set(model_params)

    batch_size = model_params['batch_size']
    next_element = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    next_element_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)
    print('ready to train!')

# -------------------------------------------------------------------------------------------------
    # 训练
    best_loss = math.inf
    loss_train, loss_test = [], []

    # learning_rate  loss_function  optimizer
    learning_rate = model_params['lr_base']
    loss_op = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

    resume_itr = 0
    early_stop_count = 0

    for step in range(resume_itr, model_params['training_steps'] + 1):
        model.train()

        # 输出平均loss
        loss_record_train = []
        loss_record_test = []

        # 记录pearson系数 r2系数 误差率 (显示在tensorboard上)
        # pearson_train = []
        # pearson_test = []
        #
        # r2_record_train = []
        # r2_record_test = []
        #

        #
        # rel_train = []
        # rel_test = []

        if step <= model_params['reduce_lr_step']:
            lr = model_params['lr_base']
        else:
            lr = model_params['lr_base'] / 10.

        for batch_x, batch_y, batch_c, batch_l, _ in next_element:
            optimizer.zero_grad()
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            batch_c = batch_c.to(device)
            batch_l = batch_l.cpu()

            pred = model(batch_x, batch_c, batch_l)
            pred, batch_y = torch.squeeze(pred), torch.squeeze(batch_y)

            pred = pred.to(device)
            batch_y = batch_y.to(device)

            loss = loss_op(pred, batch_y)
            loss_record_train.append(loss)

            # pred_temp = pred.cpu().detach().numpy()
            # batch_y_temp = batch_y.cpu().detach().numpy()

            # pearson = pearsonr(pred_temp, batch_y_temp)
            # pearson_train.append(pearson)
            #
            # r2 = r2_score(pred_temp, batch_y_temp)
            # r2_record_train.append(r2)
            #
            # rel = abs(pred - batch_y) /batch_y * 100
            # rel_train.append(rel)

            loss.backward()
            optimizer.step()

        # mean_pearson = sum(pearson_train) / len(pearson_train)
        # mean_train_r2 = sum(r2_record_train) / len(r2_record_train)
        # mean_train_rel = sum(rel_train) / len(rel_train)

        mean_train_loss = sum(loss_record_train) / len(loss_record_train)
        loss_train.append(mean_train_loss)
        print('-------------------------------------------------------------------------------------------------')
        print(f'epoch {step + 1} mean_train_loss: {mean_train_loss}')

        if mean_train_loss < best_loss:
            best_loss = mean_train_loss
            torch.save(model.state_dict(), model_params['model_dir'])
            print('Saving current best model with loss {:.3f}...'.format(best_loss))
            early_stop_count = 0
        else:
            early_stop_count += 1


        if step % model_params['eval_interval'] == 0:
            for batch_x, batch_y, batch_c, batch_l, _ in next_element_test:
                with torch.no_grad():
                    batch_x = batch_x.to(device)
                    batch_c = batch_c.to(device)
                    batch_l = batch_l.cpu()

                    pred = model(batch_x, batch_c, batch_l)

                    pred, batch_y = torch.squeeze(pred), torch.squeeze(batch_y)
                    pred = pred.to(device)
                    batch_y = batch_y.to(device)

                    loss = loss_op(pred, batch_y)

                    loss_record_test.append(loss)
            mean_test_loss = sum(loss_record_test) / len(loss_record_test)
            loss_test.append(mean_test_loss)
            print('-------------------------------------------------------------------------------------------------')
            print(f'mean_test_loss: {mean_test_loss}')

        if early_stop_count >= model_params['early_stop']:
            print('\nModel is not improving, so we halt the training session.')
            break

    torch.save(model.state_dict(), model_params['model_dir'])
    print("Model_saved, Optimization Finished!")
